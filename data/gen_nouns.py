#!/usr/bin/env python3
import json
from nltk.corpus import wordnet
from nltk.corpus import brown
import os
import re

# ChatGPT also provided heavy inspiration for the rough approach,
# though it had _many_ false starts here.
nouns = set()
singulars = set()
for word, tag in brown.tagged_words():
  if tag == 'NNS':
    singular = wordnet.morphy(word)
    if singular:
      nouns.add(word)
      singulars.add(singular)
for word, tag in brown.tagged_words():
  if tag == 'NN' and word not in singulars:
    nouns.add(word)

# At least the regex idea was mine!
_ALPHA = re.compile(r'^[A-Za-z][a-z]*$')
probable_words = (word for word in nouns if _ALPHA.match(word))

output_filename = os.path.join(os.path.dirname(__file__), 'nouns.js')
with open(output_filename, 'w') as f:
  f.write('// Generated by gen_nouns.py, and checked in so gh pages can use it\n')
  # Sort output to make this at least somewhat deterministic.
  # We'd need the original corpora to completely define it,
  # but at least this way we won't see diffs solely because of set shuffling.
  f.write(f'export nouns = {json.dumps(sorted(probable_words), indent=2)};')